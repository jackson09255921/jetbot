{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('dataset_xy_new', random_hflips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.162133, 0.054802\n",
      "2\n",
      "0.024632, 0.015023\n",
      "3\n",
      "0.018107, 0.041451\n",
      "4\n",
      "0.015714, 0.019242\n",
      "5\n",
      "0.015550, 0.021598\n",
      "6\n",
      "0.010245, 0.015614\n",
      "7\n",
      "0.008105, 0.020794\n",
      "8\n",
      "0.007742, 0.015450\n",
      "9\n",
      "0.007195, 0.013075\n",
      "10\n",
      "0.006996, 0.012671\n",
      "11\n",
      "0.005473, 0.009885\n",
      "12\n",
      "0.006741, 0.010676\n",
      "13\n",
      "0.005742, 0.009734\n",
      "14\n",
      "0.004528, 0.011392\n",
      "15\n",
      "0.004517, 0.013478\n",
      "16\n",
      "0.004048, 0.015308\n",
      "17\n",
      "0.004316, 0.009454\n",
      "18\n",
      "0.004105, 0.014004\n",
      "19\n",
      "0.003855, 0.009700\n",
      "20\n",
      "0.002997, 0.020603\n",
      "21\n",
      "0.004177, 0.011823\n",
      "22\n",
      "0.003857, 0.009080\n",
      "23\n",
      "0.002590, 0.010065\n",
      "24\n",
      "0.004390, 0.010153\n",
      "25\n",
      "0.003530, 0.008671\n",
      "26\n",
      "0.003061, 0.008547\n",
      "27\n",
      "0.003053, 0.017307\n",
      "28\n",
      "0.003354, 0.007661\n",
      "29\n",
      "0.002190, 0.007408\n",
      "30\n",
      "0.004152, 0.011062\n",
      "31\n",
      "0.002874, 0.009388\n",
      "32\n",
      "0.002069, 0.008641\n",
      "33\n",
      "0.002307, 0.009727\n",
      "34\n",
      "0.001826, 0.008477\n",
      "35\n",
      "0.002741, 0.009628\n",
      "36\n",
      "0.003296, 0.006090\n",
      "37\n",
      "0.002857, 0.007627\n",
      "38\n",
      "0.002293, 0.008683\n",
      "39\n",
      "0.003057, 0.009779\n",
      "40\n",
      "0.002227, 0.008292\n",
      "41\n",
      "0.003556, 0.018136\n",
      "42\n",
      "0.003321, 0.010097\n",
      "43\n",
      "0.001929, 0.014586\n",
      "44\n",
      "0.002435, 0.010203\n",
      "45\n",
      "0.001590, 0.009817\n",
      "46\n",
      "0.002206, 0.007845\n",
      "47\n",
      "0.002563, 0.008311\n",
      "48\n",
      "0.001661, 0.008474\n",
      "49\n",
      "0.001380, 0.011521\n",
      "50\n",
      "0.001613, 0.008556\n",
      "51\n",
      "0.001249, 0.006261\n",
      "52\n",
      "0.002630, 0.011915\n",
      "53\n",
      "0.002266, 0.010354\n",
      "54\n",
      "0.002365, 0.009013\n",
      "55\n",
      "0.003515, 0.009177\n",
      "56\n",
      "0.002241, 0.007673\n",
      "57\n",
      "0.001968, 0.010600\n",
      "58\n",
      "0.002649, 0.006750\n",
      "59\n",
      "0.002429, 0.005672\n",
      "60\n",
      "0.002025, 0.008156\n",
      "61\n",
      "0.002034, 0.010177\n",
      "62\n",
      "0.002006, 0.007800\n",
      "63\n",
      "0.001671, 0.008836\n",
      "64\n",
      "0.002232, 0.009821\n",
      "65\n",
      "0.003285, 0.009931\n",
      "66\n",
      "0.002763, 0.006569\n",
      "67\n",
      "0.002460, 0.007121\n",
      "68\n",
      "0.002309, 0.012786\n",
      "69\n",
      "0.001627, 0.008806\n",
      "70\n",
      "0.001394, 0.011113\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 70\n",
    "BEST_MODEL_PATH = 'best_steering_model_xy_new_v2.pth'\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "round = 1\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    print(round)\n",
    "    round += 1\n",
    "    print('%f, %f' % (train_loss, test_loss))\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
